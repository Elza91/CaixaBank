{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 0. Tabla de contenidos\n\n# 1. Introducción\n\nCada día, millones de datos se suben a la red sin darnos cuenta. ¿Una opinión? ¿Una sugerencia? Estos datos tienen repercusiones hacia el area que están focalizados. La Bolsa no se ha quedado atrás. Compañías tecnológicas de todo el mundo han visto como las opiniones en RRSS como Twitter puede marcar las futuras acciones de los clientes en las acciones de una compañía, no obstante ejecutivos de iSentium, por ejemplo, comentan que Twitter puede aportar información útil a los inversores, pero que no deberían confiar únicamente en ella a la hora de invertir [1]\n\nNuestro caso de estudio está enfocado al mercado de valores IBEX35. \n\nEl IBEX 35 es el índice oficial de la bolsa española compuesto por las 35 empresas más negociadas del mercado. Este índice nos muestra en tiempo real si los precios en bolsa están subiendo o bajando, por lo que permite medir el comportamiento de este conjunto de acciones.\n\nEl IBEX35 sirve como punto de referencia para los inversores del mercado español. La rentabilidad de este índice es el objetivo a batir por los gestores.\n\nPor lo tanto, la modelización de las dinámicas de este tipo de índices resultan esenciales para la toma de decisiones por parte de todas las entidades bursátiles.\n\nPor lo tanto los objetivos de este estudio son:\n\n✅ Task 1 → Análisis de sentimientos de tweets entre el periodo 2015 hasta la actualidad\n\n✅ Task 2 → ¿Tweeter afecta a la toma de decisiones en la Bolsa IBEX35?\n\n✅ Task 3 → Modelo predictivo de IBEX35\n\n\n# 2. Preparación de los datos:\n\nLa organización de este estudio nos ha proporcionado tres datasets. Dos tienen las mismas características pero uno no tiene el target debido a que se tratarán como datos nuevos. El dataset que resta es un set de tweets con estructura muy diferentes a los otros dos.\n\n## 2.1 Características de los datos:\n\n- `train.csv` - Consta de 6554 entradas y 8 características.\n\n    - `Date` : Día al que hacen referencia los datos presentados.\n    - `Open`: Precio de apertura de ese día.\n    - `High`: Precio máximo alcanzado durante ese día.\n    - `Low`: Precio mínimo alcanzado durante ese día.\n    - `Close`: Precio de cierre de ese día ajustado por splits.\n    - `Adj Close`: Precio de cierre ajustado por splits y distribuciones de dividendos o plusvalías.\n    - `Volume`: El número físico de acciones negociadas del índice bursátil.\n    - `Target`: Esta es la variable a predecir. Es una variable binaria.\n        - `1`: Indica que el precio de cierre tres días adelante será más alto que el precio de cierre actual.\n        - `0`: Indica que el precio de cierre tres días adelante será igual o menor al precio actual.\n\n            \n- `test.csv` - Consta de 726 entradas y 7 características.\n\n- `tweets_from2015_#Ibex35.csv`: Contiene los tweets públicos que contienen el hashtag #Ibex35 desde el año 2015 que han recibido más de dos likes y de dos retweets.\n\n## 2.2 Librerías:\n\nAhora vamos a importar todas las librerías necesarias para hacer este estudio:","metadata":{}},{"cell_type":"code","source":"import plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.cluster import KMeans\n\nfrom scipy import stats\nfrom scipy.stats import norm\n\nimport os\nimport gc","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:06.196239Z","iopub.execute_input":"2022-05-28T21:14:06.196680Z","iopub.status.idle":"2022-05-28T21:14:06.208348Z","shell.execute_reply.started":"2022-05-28T21:14:06.196643Z","shell.execute_reply":"2022-05-28T21:14:06.206639Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport os\nimport gc\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import backend as K\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import RobustScaler, LabelEncoder\nfrom sklearn.metrics import confusion_matrix\n\nfrom scipy.stats import zscore\nfrom scipy.stats import iqr\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\n\n\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:25:25.499028Z","iopub.execute_input":"2022-05-28T21:25:25.499751Z","iopub.status.idle":"2022-05-28T21:25:25.507409Z","shell.execute_reply.started":"2022-05-28T21:25:25.499718Z","shell.execute_reply":"2022-05-28T21:25:25.506769Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nimport re\nimport nltk\nnltk.download('vader_lexicon')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('omw-1.4')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport unicodedata\nsentiment_i_a = SentimentIntensityAnalyzer()\nimport string\nfrom nltk.corpus import subjectivity\nfrom nltk.sentiment import SentimentAnalyzer\nfrom nltk.sentiment.util import *\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-28T21:14:06.874562Z","iopub.execute_input":"2022-05-28T21:14:06.875855Z","iopub.status.idle":"2022-05-28T21:14:06.901648Z","shell.execute_reply.started":"2022-05-28T21:14:06.875789Z","shell.execute_reply":"2022-05-28T21:14:06.900781Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"## 2.3 Leer los datos:\n\n### 2.3.1 Train y Test:\n\nComo hemos mencionado el dataset `train` y `test` son iguales menos el target que está presente en el dataset train solamente. Vamos a leerlos:","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('../input/ibex98/train.csv')\ndf_test = pd.read_csv('../input/ibex98/test_x(1).csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:08.402400Z","iopub.execute_input":"2022-05-28T21:14:08.403311Z","iopub.status.idle":"2022-05-28T21:14:08.431551Z","shell.execute_reply.started":"2022-05-28T21:14:08.403267Z","shell.execute_reply":"2022-05-28T21:14:08.430215Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Vemos que el dataset `train` está compuesto por 6554 entradas y 8 caractarísticas. Vemos que hay presencia de datos faltantes, por lo que se deberá corregir posteriormente.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:09.863310Z","iopub.execute_input":"2022-05-28T21:14:09.864770Z","iopub.status.idle":"2022-05-28T21:14:09.879201Z","shell.execute_reply.started":"2022-05-28T21:14:09.864700Z","shell.execute_reply":"2022-05-28T21:14:09.878114Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"df_test.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:10.469769Z","iopub.execute_input":"2022-05-28T21:14:10.470544Z","iopub.status.idle":"2022-05-28T21:14:10.484948Z","shell.execute_reply.started":"2022-05-28T21:14:10.470491Z","shell.execute_reply":"2022-05-28T21:14:10.483735Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### 2.3.3 Tweet:\n\nEste dataset es diferente en su estructura. Como se puede apreciar consta de 9801 entradas y 3 características. Como podemos observar hay una característica que es similar al dataset train y test, `tweetDate`. Aunque el formato es diferente, podemos modificarlo para que tengan el mismo formato y si es una columna principal (es decir, que determina el conjunto del dataset y por ello no tiene índices duplicados) podemos unirla a la columna principal de train, `Date`. De esta manera podremos ver la implicación de los tweets en las decisiones de compra venta de nuestro inversores en el IBEX35.","metadata":{}},{"cell_type":"code","source":"tweet = pd.read_csv('../input/ibex98/tweets_from2015_Ibex35.csv')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:11.662084Z","iopub.execute_input":"2022-05-28T21:14:11.662444Z","iopub.status.idle":"2022-05-28T21:14:11.725607Z","shell.execute_reply.started":"2022-05-28T21:14:11.662415Z","shell.execute_reply":"2022-05-28T21:14:11.724282Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tweet.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:12.262558Z","iopub.execute_input":"2022-05-28T21:14:12.263574Z","iopub.status.idle":"2022-05-28T21:14:12.277157Z","shell.execute_reply.started":"2022-05-28T21:14:12.263518Z","shell.execute_reply":"2022-05-28T21:14:12.276278Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"## 2.4 Preprocesamiento de los datos.\n\n### 2.4.1 Datos duplicados:\n\n#### 2.4.1.1 Train y Test:\n\n¿Hay presencia de datos duplicados en nuestro dataset?","metadata":{}},{"cell_type":"code","source":"print('Número de datos duplicados: ')\nprint('Conjunto df_train:\\t', train[train.duplicated()==True].shape[0])\nprint('Conjunto df_test:\\t', df_test[df_test.duplicated()==True].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:13.388690Z","iopub.execute_input":"2022-05-28T21:14:13.389866Z","iopub.status.idle":"2022-05-28T21:14:13.404320Z","shell.execute_reply.started":"2022-05-28T21:14:13.389812Z","shell.execute_reply":"2022-05-28T21:14:13.403141Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"Como se esperaba, no hay datos duplicados en dichos datasets\n\n#### 2.4.1.2 Tweet:\n\n¿Hay presencia de datos dupicados en este dataset?","metadata":{}},{"cell_type":"code","source":"print('Número de datos duplicados: ')\nprint('Conjunto df_train:\\t', tweet[tweet.duplicated()==True].shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:14.533709Z","iopub.execute_input":"2022-05-28T21:14:14.534143Z","iopub.status.idle":"2022-05-28T21:14:14.552712Z","shell.execute_reply.started":"2022-05-28T21:14:14.534106Z","shell.execute_reply":"2022-05-28T21:14:14.551560Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"Hay 16 datos duplicados. Por lo que se deberán eliminarlos.","metadata":{}},{"cell_type":"markdown","source":"### 2.4.2 Datos faltantes:\n\nLos datos faltantes son un problema para modelar el algoritmo, por lo que se debe de tener en cuenta cómo tratarlos. \n\n#### 2.4.2.1 Train y Test:\n\nPrimero nos fijaremos en el dataset `train`. Vemos que sí hay presencia de datos faltantes por lo que, como constituyen el 2% de los datos, vamos a proceder a eliminarlos. ","metadata":{}},{"cell_type":"code","source":"train = train.dropna()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:16.388717Z","iopub.execute_input":"2022-05-28T21:14:16.389432Z","iopub.status.idle":"2022-05-28T21:14:16.396936Z","shell.execute_reply.started":"2022-05-28T21:14:16.389396Z","shell.execute_reply":"2022-05-28T21:14:16.396217Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"A continuación, expongo el dataset sin datos faltantes. Vemos que se ha reducido a 6421 entradas.","metadata":{}},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:17.872807Z","iopub.execute_input":"2022-05-28T21:14:17.873379Z","iopub.status.idle":"2022-05-28T21:14:17.887008Z","shell.execute_reply.started":"2022-05-28T21:14:17.873337Z","shell.execute_reply":"2022-05-28T21:14:17.885543Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"En el caso del `test` se puede observar que no hay presencia de datos faltantes, por lo que se queda con las mismas dimensiones.\n\n#### 2.4.2.2 Tweet:\n\nEn este dataset sí hay la presencia de datos faltantes. Por lo que eliminaremos dichas filas, quedándos con 4 rows menos.","metadata":{}},{"cell_type":"code","source":"tweet[tweet.tweetDate.notnull() ==False]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:19.193632Z","iopub.execute_input":"2022-05-28T21:14:19.193962Z","iopub.status.idle":"2022-05-28T21:14:19.212490Z","shell.execute_reply.started":"2022-05-28T21:14:19.193938Z","shell.execute_reply":"2022-05-28T21:14:19.211423Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"tweet[tweet.text.notnull() ==False]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:19.826271Z","iopub.execute_input":"2022-05-28T21:14:19.827366Z","iopub.status.idle":"2022-05-28T21:14:19.840034Z","shell.execute_reply.started":"2022-05-28T21:14:19.827322Z","shell.execute_reply":"2022-05-28T21:14:19.839303Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"tweet = tweet.drop(index=[6931,1070,9667,9634])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:37.239759Z","iopub.execute_input":"2022-05-28T21:14:37.240754Z","iopub.status.idle":"2022-05-28T21:14:37.247679Z","shell.execute_reply.started":"2022-05-28T21:14:37.240710Z","shell.execute_reply":"2022-05-28T21:14:37.246645Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"tweet = tweet.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:37.952923Z","iopub.execute_input":"2022-05-28T21:14:37.953415Z","iopub.status.idle":"2022-05-28T21:14:37.960397Z","shell.execute_reply.started":"2022-05-28T21:14:37.953383Z","shell.execute_reply":"2022-05-28T21:14:37.959235Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"tweet = tweet.drop(['index','handle'],axis=1)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:39.333457Z","iopub.execute_input":"2022-05-28T21:14:39.334256Z","iopub.status.idle":"2022-05-28T21:14:39.341652Z","shell.execute_reply.started":"2022-05-28T21:14:39.334217Z","shell.execute_reply":"2022-05-28T21:14:39.340405Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## 2.5 Tratamiento Outliers:\n\nVamos a ver si hay outliers y si los hay, vamos a eliminarlo añadiendo un informe final de dónde están y el tanto por ciento que implica eliminarlos. No añadiremos la característica `Volume`porque está muy fuera del rango de las otras y no se pueden ver correctament (aunque la gráfica sea interactiva):","metadata":{}},{"cell_type":"code","source":"df_train = train","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:41.091836Z","iopub.execute_input":"2022-05-28T21:14:41.092731Z","iopub.status.idle":"2022-05-28T21:14:41.098499Z","shell.execute_reply.started":"2022-05-28T21:14:41.092668Z","shell.execute_reply":"2022-05-28T21:14:41.096964Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:14:53.940670Z","iopub.execute_input":"2022-05-28T21:14:53.941201Z","iopub.status.idle":"2022-05-28T21:14:53.956680Z","shell.execute_reply.started":"2022-05-28T21:14:53.941161Z","shell.execute_reply":"2022-05-28T21:14:53.955695Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure()\n# Use x instead of y argument for horizontal plot\n\nx0 = df_train['Open']\nx1 = df_train['High']\nx2 = df_train['Low']\nx3 = df_train['Close']\nx4 = df_train['Adj Close']\n#x5 = df_train['Volume']\n\nx6 = df_train['Target']\n\nfig.update_layout(title_text='Box plot of variables')\n\nfig.add_trace(go.Box(x=x0, name= \"Open\"))\nfig.add_trace(go.Box(x=x1, name = \"High\"))\nfig.add_trace(go.Box(x=x2, name = \"Low\"))\nfig.add_trace(go.Box(x=x3, name = \"Close\"))\nfig.add_trace(go.Box(x=x4, name = \"Adj Close\"))\n#fig.add_trace(go.Box(x=x5, name = \"Volume\"))\nfig.add_trace(go.Box(x=x6, name = \"Target\"))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:00.748613Z","iopub.execute_input":"2022-05-28T21:15:00.749063Z","iopub.status.idle":"2022-05-28T21:15:00.807397Z","shell.execute_reply.started":"2022-05-28T21:15:00.749009Z","shell.execute_reply":"2022-05-28T21:15:00.806525Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a ver un informe de los outliers, donde están y el tanto por ciento que representa eliminarlos. Como vemos no supera el 10% por tanto, vale la pena eliminarlos.","metadata":{}},{"cell_type":"code","source":"df_train = df_train.drop(outlier_list,axis=0).reset_index(drop = True)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T20:59:02.947003Z","iopub.status.idle":"2022-05-28T20:59:02.947848Z","shell.execute_reply.started":"2022-05-28T20:59:02.947685Z","shell.execute_reply":"2022-05-28T20:59:02.947704Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a ver como queda el target después del tratamiento de outliers","metadata":{}},{"cell_type":"code","source":"df_train[df_train['Target']==0].shape \ndf_train[df_train['Target']==1].shape \n\n\nlabels = ['0','1']\nvalues = [3033,3388]\ncolors = ['green','lightgreen','gold', 'mediumturquoise', 'darkorange', 'lightgreen']\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.4)])\n\nfig.update_traces(textposition='inside', textinfo='percent+label')\nfig.update_layout(margin = dict(t=25, l=0, r=0, b=0))\nfig.update_traces(marker=dict(colors=colors))\nfig.update_layout(\n    title_text=\"Distribución del Target despúes del tratamiento de outliers\",\n    annotations=[dict(text='sin Outlier', x=0.50, y=0.5, font_size=20, showarrow=False)])\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:18.920657Z","iopub.execute_input":"2022-05-28T21:15:18.921270Z","iopub.status.idle":"2022-05-28T21:15:19.205328Z","shell.execute_reply.started":"2022-05-28T21:15:18.921229Z","shell.execute_reply":"2022-05-28T21:15:19.204339Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"## 2.6 Visualización:\n\nComo bien es sabido, los gráficos de IBEX35 y otros mercados de valores aparecen representadas con las velas japonesas. Estas dan una visión muy clara de la tendencia del índice. Puede indicar tiempos intradía como movimientos por minuto, hora... hasta movimientos por meses, trimestres... por lo que se debe especificar qué tiempos se quiere marcar. En nuetro caso, cada vela será un día. La representación general será la siguiente:","metadata":{}},{"cell_type":"code","source":"import plotly.graph_objects as go\n\nfig = go.Figure(data=go.Ohlc(x=train['Date'],\n        open=train['Open'],\n        high=train['High'],\n        low=train['Low'],\n        close=train['Close']))\n        \nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:21.343611Z","iopub.execute_input":"2022-05-28T21:15:21.344074Z","iopub.status.idle":"2022-05-28T21:15:21.403601Z","shell.execute_reply.started":"2022-05-28T21:15:21.344020Z","shell.execute_reply":"2022-05-28T21:15:21.402403Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"Como podemos observar, la gráfica marca dos colores (rojo para tendencia bajista y verde para tendencia alcista). A priori no vemos un patrón predeterminado, pero sí que la tendencia neutral desde 2015 nos da a entender que la crisis económica tuvo repercusión en este mercado de valores.\n\nPara invertir en un valor, hay diferentes indicadores que nos ayudan a ver tendencias, cambios de tendencia... etc. El más sencillo es media móvil simple (`SMA`). Éste nos indica el precio promedio de un activo durante un número particular de períodos. En nuestro caso hemos creado 3 periodos de tiempo: 5 días, 20 días y 50 días. A continuación veremos cómo se comportan estos indicadores y si realmente sirven para ver cambios de tendencia.","metadata":{}},{"cell_type":"code","source":"train['SMA5'] = train.Close.rolling(5).mean()\ntrain['SMA20'] = train.Close.rolling(20).mean()\ntrain['SMA50'] = train.Close.rolling(50).mean()\n\nfig = go.Figure(data=[go.Ohlc(x=train['Date'],\n            open=train['Open'],\n            high=train['High'],\n            low=train['Low'],\n            close=train['Close'], name = \"OHLC\"),\n            go.Scatter(x=train.Date, y=train.SMA5, line=dict(color='orange', width=1), name=\"SMA5\"),\n            go.Scatter(x=train.Date, y=train.SMA20, line=dict(color='green', width=1), name=\"SMA20\"),\n            go.Scatter(x=train.Date, y=train.SMA50, line=dict(color='blue', width=1), name=\"SMA50\")])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:25.531456Z","iopub.execute_input":"2022-05-28T21:15:25.532182Z","iopub.status.idle":"2022-05-28T21:15:25.721195Z","shell.execute_reply.started":"2022-05-28T21:15:25.532125Z","shell.execute_reply":"2022-05-28T21:15:25.720216Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Para principiantes, es una forma de empezar a ver las tendencias y los cambios de éstas, pero vamos un poco más allá. Ahora usaremos las `EMA`. Estas se diferencian de las `SMA`en que da más peso a los datos de precios más recientes, lo que hace que reaccione más rápidamente a los cambios de precios. En nuestro caso usaremos dos peridod: 5 días y 20 días. ","metadata":{}},{"cell_type":"code","source":"train['EMA3'] = train.Close.ewm(span=3, adjust=False).mean()\ntrain['EMA5'] = train.Close.ewm(span=5, adjust=False).mean()\ntrain['EMA20'] = train.Close.ewm(span=20, adjust=False).mean()\n\ndf_test['EMA5'] = df_test.Close.ewm(span=5, adjust=False).mean()\ndf_test['EMA20'] = df_test.Close.ewm(span=20, adjust=False).mean()\n\nfig = go.Figure(data=[go.Ohlc(x=train['Date'],\n        open=train['Open'],\n        high=train['High'],\n        low=train['Low'],\n        close=train['Close'], name = \"OHLC\"),\n        go.Scatter(x=train.Date, y=train.EMA5, line=dict(color='orange', width=1), name=\"EMA3\"),\n        go.Scatter(x=train.Date, y=train.EMA20, line=dict(color='green', width=1), name=\"EMA20\")])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:29.687392Z","iopub.execute_input":"2022-05-28T21:15:29.687755Z","iopub.status.idle":"2022-05-28T21:15:29.831346Z","shell.execute_reply.started":"2022-05-28T21:15:29.687722Z","shell.execute_reply":"2022-05-28T21:15:29.830120Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":"Como podemos comprobar las `EMA` dan mejor intuición en el cambio de precios. Como el target que los patrocinadores nos ha proporcionado tiene que ver con el comportamiento de 3 días anteriores usaremos usa EMA(3) para ver su comportamiento. No obstante, cada tres días es difícil poder ver un cambio significativo de tendencia. Por eso hemos incorporado EMA(5) y EMA(20), ya que representan la semana y el mes (20 días laborables aproximadamente al mes).\n\nA continuación, incorporaremos la característica `Volumen` para ver el número físico de acciones negociadas del índice bursátil. Como se puede observar, a finales de `2010` los valores de esta característica son significativos, mientras que anteriormente no. ","metadata":{}},{"cell_type":"code","source":"# Create figure with secondary y-axis\nfig = make_subplots(specs=[[{\"secondary_y\": True}]])\n\n# include candlestick with rangeselector\nfig.add_trace(go.Candlestick(x=train['Date'],\n                open=train['Open'], high=train['High'],\n                low=train['Low'], close=train['Close']),\n               secondary_y=True)\n\n# include a go.Bar trace for volumes\nfig.add_trace(go.Bar(x=train['Date'], y=train['Volume']),\n               secondary_y=False)\n\nfig.layout.yaxis2.showgrid=False\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:32.834170Z","iopub.execute_input":"2022-05-28T21:15:32.834643Z","iopub.status.idle":"2022-05-28T21:15:32.949832Z","shell.execute_reply.started":"2022-05-28T21:15:32.834602Z","shell.execute_reply":"2022-05-28T21:15:32.948668Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"markdown","source":"A continuacuón, vamos a deficir más indicadores para ver los soportes y techos de los datos a medida que va transcurriendo el tiempo. Para ello, defiremos las bandas de bollinger. `Las bandas de bollinger` miden la volatilidad del mercado y proporciona una gran cantidad de información muy útil para tomar decisiones de compra y venta de activos financieros. Básicamente, si son estrechas indica que el valor está estancado y puede producirse un cambio de tendencia. También nos marcan las tendencias del valor IBEX35 (en nuestro caso). Como podemos ver hay tres indicadores:\n- `B_MA`: indica la banda media conformada por la media de los precios `High` + `Low`+ `Close`.\n- `BU`: indica el techo del precio.\n- `BL` : indica el soporte del precio.\n\nTodo esto para un periodo de tiempo determinado. Vamos a verlo.","metadata":{}},{"cell_type":"code","source":"def bollinger_bands(df, n, m):\n    # takes dataframe on input\n    # n = smoothing length\n    # m = number of standard deviations away from MA\n    \n    #typical price\n    TP = (df['High'] + df['Low'] + df['Close']) / 3\n    # but we will use Adj close instead for now, depends\n    \n    data = TP\n    #data = df['Adj Close']\n    \n    # takes one column from dataframe\n    B_MA = pd.Series((data.rolling(n, min_periods=n).mean()), name='B_MA')\n    sigma = data.rolling(n, min_periods=n).std() \n    \n    BU = pd.Series((B_MA + m * sigma), name='BU')\n    BL = pd.Series((B_MA - m * sigma), name='BL')\n    \n    df = df.join(B_MA)\n    df = df.join(BU)\n    df = df.join(BL)\n    \n    return df\n\ndf = bollinger_bands(train, 20, 2)\ndf.tail()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:42.482913Z","iopub.execute_input":"2022-05-28T21:15:42.483725Z","iopub.status.idle":"2022-05-28T21:15:42.522641Z","shell.execute_reply.started":"2022-05-28T21:15:42.483685Z","shell.execute_reply":"2022-05-28T21:15:42.521411Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"fig = go.Figure(data=[go.Ohlc(x=train['Date'],\n        open=train['Open'],\n        high=train['High'],\n        low=train['Low'],\n        close=train['Close'], name = \"OHLC\"),\n        go.Scatter(x=train.Date, y=train['Adj Close'], line=dict(color='orange', width=1), name=\"Adj Close\"),\n        ])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:43.738027Z","iopub.execute_input":"2022-05-28T21:15:43.738493Z","iopub.status.idle":"2022-05-28T21:15:43.808817Z","shell.execute_reply.started":"2022-05-28T21:15:43.738454Z","shell.execute_reply":"2022-05-28T21:15:43.808236Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"En este punto, ya tenemos una intuición global de nuestro dataset, sus cambios de tendencias y el análisis técnica para poder verlas. \n\n## 2.7 Descripción estadística de los datos\n\nComo podemos observar todos los datos presentan la misma escala numérica menos `Volumen`y `Target`. \nHemos añadido tres estadísticos más para ver si conforman datos sesgados y no simétricos. Para ello nos ayudaremos de un `mapa de color` para ver los datos positivos en verde y negativos en rojo.","metadata":{}},{"cell_type":"code","source":"# mapa de color\ndef colour_map(value):\n    if value < 0:\n        color = 'red'\n    elif value > 0:\n        color = 'green'\n    else:\n        color = \"black\"\n        \n    return \"color: %s\" %color","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:47.464535Z","iopub.execute_input":"2022-05-28T21:15:47.464909Z","iopub.status.idle":"2022-05-28T21:15:47.470375Z","shell.execute_reply.started":"2022-05-28T21:15:47.464880Z","shell.execute_reply":"2022-05-28T21:15:47.468885Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"stats = df_train.describe()\nstats.loc['var'] = df_train.var().tolist()\nstats.loc['skew'] = df_train.skew().tolist()\nstats.loc['kurt'] = df_train.kurtosis().tolist()\nstats.style.applymap(colour_map)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:48.326264Z","iopub.execute_input":"2022-05-28T21:15:48.326662Z","iopub.status.idle":"2022-05-28T21:15:48.480934Z","shell.execute_reply.started":"2022-05-28T21:15:48.326633Z","shell.execute_reply":"2022-05-28T21:15:48.480379Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"markdown","source":"Como se observa la varianza es enorme en todas las características. También hay asimetría y sesgo en los datos, por lo que `no conforma una distibución Gaussiana`. Esto lo vemos más adeltante con un gráfico grobal de todas las variables por separado.","metadata":{}},{"cell_type":"code","source":"fig = go.Figure(data=[go.Scatter(x = df['Date'], y=df['Adj Close'], name = \"Adj Close\"),\n        go.Scatter(x = df['Date'],y=df['BU'], line=dict(color='orange', width=1), name=\"BU\"),\n        go.Scatter(x = df['Date'],y=df['BL'], line=dict(color='pink', width=2), name=\"BL\"),\n        go.Scatter(x = df['Date'],y=df['B_MA'], line=dict(color='red', width=1), name=\"B_MA\")\n        ])\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:51.052652Z","iopub.execute_input":"2022-05-28T21:15:51.053623Z","iopub.status.idle":"2022-05-28T21:15:51.166473Z","shell.execute_reply.started":"2022-05-28T21:15:51.053577Z","shell.execute_reply":"2022-05-28T21:15:51.164360Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"Como podemos observar, las bandas dan cuenta de la tendencia y cambio de tendencia del mercado. Son una fuente de información muy útil para la compra venta de las acciones. Gracias a ellas, por ejemplo, podemos ver que hay un suelo en `6k` que indicaría que si en un futuro volviera los precios por ese suelo habría la posibilidad de comprar ya que subiría. No obstante, para los más profesionales, hay un término usado `break support` que implica que podría romper ese suelo y el valor continuar bajando hasta el siguiente suelo (en nuestro caso es en `3.6k`). \n\nPara rizar más el rizo, podríamos incluso creear señales de compra y venta. Las más sencillas son las que presentaremos a continuación:\n\n- Si `High` > `BU` : sell signal weekly\n- Si `Low` < `BL`: buy signal weekly\n\nAunque sé que el estudio indica un target definido en datos de 3 días anterior, no tiene nada que ver con estas señales de compra/venta. El target nos dice que si el precio de cierre tres días adelante es más alto que el precio de cierre actual indicar con `1` ya que sería una señal de una posible tendencia alcista. De lo contraria indicaría un posible cambio de tendencia a tendencia bajista por lo que venderíamos las acciones en este punto ya que su valos comenzaría a caer. No obstante, como he dicho antes 3 días no son suficientes para poder hacer una señal fide-digna. Ahora vamos a ver la gráfica con la incorporación de las señales. Como vemos están perfectamente marcadas. Un proyecto a futuro sería crear señales futuras para poder comprar/vender.","metadata":{}},{"cell_type":"code","source":"def add_signal(df):\n    # adds two columns to dataframe with buy and sell signals\n    buy_list = []\n    sell_list = []\n    \n    for i in range(len(df['Close'])):\n        #if df['Close'][i] > df['BU'][i]:           # sell signal     daily\n        if df['High'][i] > df['BU'][i]:             # sell signal     weekly\n            buy_list.append(np.nan)\n            sell_list.append(df['Close'][i])\n        #elif df['Close'][i] < df['BL'][i]:         # buy signal      daily\n        elif df['Low'][i] < df['BL'][i]:            # buy signal      weekly\n            buy_list.append(df['Close'][i])\n            sell_list.append(np.nan)  \n        else:\n            buy_list.append(np.nan)\n            sell_list.append(np.nan)\n         \n    buy_list = pd.Series(buy_list, name='Buy')\n    sell_list = pd.Series(sell_list, name='Sell')\n        \n    df = df.join(buy_list)\n    df = df.join(sell_list)        \n     \n    return df\n\ndef plot_signals(df, ticker):\n    # plot price\n    plt.figure(figsize=(15,5))\n    plt.plot(df['Date'], df['Adj Close'])\n    plt.title('Price chart (Adj Close) ' + str(ticker))\n    plt.show()\n\n    # plot  values and significant levels\n    plt.figure(figsize=(15,5))\n    plt.title('Bollinger Bands chart ' + str(ticker))\n    plt.plot(df['Date'], df['Adj Close'], label='Adj Close')\n\n    plt.plot(df['Date'], df['High'], label='High', alpha=0.3)\n    plt.plot(df['Date'], df['Low'], label='Low', alpha=0.3)\n\n    plt.plot(df['Date'], df['BU'], label='B_Upper', alpha=0.3)\n    plt.plot(df['Date'], df['BL'], label='B_Lower', alpha=0.3)\n    plt.plot(df['Date'], df['B_MA'], label='B_SMA', alpha=0.3)\n    plt.fill_between(df['Date'], df['BU'], df['BL'], color='grey', alpha=0.1)\n\n    plt.scatter(df['Date'], df['Buy'], label='Buy', marker='^')\n    plt.scatter(df['Date'], df['Sell'], label='Sell', marker='v')\n\n    plt.legend()\n\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:53.301018Z","iopub.execute_input":"2022-05-28T21:15:53.301505Z","iopub.status.idle":"2022-05-28T21:15:53.324297Z","shell.execute_reply.started":"2022-05-28T21:15:53.301465Z","shell.execute_reply":"2022-05-28T21:15:53.323338Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"#### RESAMPLING TO WEEKLY TO CLEAN NOISE\nagg_dict = {'Open': 'first',\n          'High': 'max',\n          'Low': 'min',\n          'Close': 'last',\n          'Adj Close': 'last',\n          'Volume': 'mean'}\n\n# resampled dataframe\n# 'W' means weekly aggregation\ndf['Date'] = pd.to_datetime(df['Date'].apply(lambda x: x.split()[0]), format='%Y-%m-%d') \ndf.set_index('Date', inplace=True)\ndf_agg = df.resample('W').agg(agg_dict)\ndf_agg = df.reset_index()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:54.963916Z","iopub.execute_input":"2022-05-28T21:15:54.964359Z","iopub.status.idle":"2022-05-28T21:15:55.013006Z","shell.execute_reply.started":"2022-05-28T21:15:54.964318Z","shell.execute_reply":"2022-05-28T21:15:55.012083Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"df_agg = add_signal(df_agg)\nplot_signals(df_agg, \"IBEX\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:15:55.682353Z","iopub.execute_input":"2022-05-28T21:15:55.682802Z","iopub.status.idle":"2022-05-28T21:15:56.454254Z","shell.execute_reply.started":"2022-05-28T21:15:55.682765Z","shell.execute_reply":"2022-05-28T21:15:56.453086Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Una vez tengamos toda esta información, vamos a ver cada característica por separado.\n\n## 2.8 EDA\n\n### 2.8.1 Open:\n\nOpen indica el precio de apertura de ese día.","metadata":{}},{"cell_type":"code","source":"import matplotlib.gridspec as gridspec\nfrom matplotlib.ticker import MaxNLocator\nfrom scipy import stats\nfrom scipy.stats import norm\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:16:29.666824Z","iopub.execute_input":"2022-05-28T21:16:29.667294Z","iopub.status.idle":"2022-05-28T21:16:29.766211Z","shell.execute_reply.started":"2022-05-28T21:16:29.667255Z","shell.execute_reply":"2022-05-28T21:16:29.765372Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"# setting some globl config\nplt.style.use('fivethirtyeight')\ncust_color = ['#fdc029',\n'#f7c14c',\n'#f0c268',\n'#e8c381',\n'#dfc498',\n'#d4c5af',\n'#c6c6c6',\n'#a6a6a8',\n'#86868a',\n'#68686d',\n'#4b4c52',\n'#303138',\n'#171820',\n]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:16:32.080600Z","iopub.execute_input":"2022-05-28T21:16:32.081196Z","iopub.status.idle":"2022-05-28T21:16:32.086524Z","shell.execute_reply.started":"2022-05-28T21:16:32.081158Z","shell.execute_reply":"2022-05-28T21:16:32.085723Z"},"trusted":true},"execution_count":52,"outputs":[]},{"cell_type":"code","source":"\nplt.rcParams[\"figure.figsize\"] = (14, 14)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:20.847908Z","iopub.execute_input":"2022-05-28T21:18:20.848361Z","iopub.status.idle":"2022-05-28T21:18:20.853435Z","shell.execute_reply.started":"2022-05-28T21:18:20.848322Z","shell.execute_reply":"2022-05-28T21:18:20.852592Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"def eli_plot(df, feature, title):\n    \n    # Creating a customized chart. and giving in figsize and everything.\n    \n    fig = plt.figure(constrained_layout=True)\n    \n    # creating a grid of 3 cols and 3 rows.\n    \n    grid = gridspec.GridSpec(ncols=3, nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    \n    ax1 = fig.add_subplot(grid[0, :2])\n    \n    # Set the title.\n    \n    ax1.set_title('Histogram')\n    \n    # plot the histogram.\n    \n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 fit=norm,\n                  hist_kws={\n                 'rwidth': 0.85,\n                 'edgecolor': 'black',\n                 'linewidth':.5,\n                 'alpha': 0.8},\n                 ax=ax1,\n                 color=cust_color[0])\n    \n    ax1.axvline(df.loc[:, feature].mean(), color='Green', linestyle='dashed', linewidth=3)\n\n    min_ylim, max_ylim = plt.ylim()\n    ax1.text(df.loc[:, feature].mean()*1.25, max_ylim*0.85, \n             'Mean: {:.2f}'.format(df.loc[:, feature].mean()), \n             color='Green', fontsize='12',\n             bbox=dict(boxstyle='round',facecolor='red', alpha=0.5))\n    ax1.legend(labels=['Actual','Normal'])\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=12))\n    \n    \n    ax1.annotate(\n    # Label and coordinate\n    'Unexpected Spike here!', xy=(10000, 0.00025),xytext=(10500, 0.0004) ,\n    horizontalalignment=\"center\",\n    # Custom arrow\n    arrowprops=dict(arrowstyle='simple',lw=1, color='black'), fontsize=8\n    )\n\n    # customizing the QQ_plot.\n    \n    ax2 = fig.add_subplot(grid[1, :2])\n    \n    # Set the title.\n    \n    ax2.set_title('Probability Plot')\n    \n    # Plotting the QQ_Plot.\n    stats.probplot(df.loc[:, feature],\n                   plot=ax2)\n    ax2.get_lines()[0].set_markerfacecolor('#e74c3c')\n    ax2.get_lines()[0].set_markersize(12.0)\n    ax2.xaxis.set_major_locator(MaxNLocator(nbins=16))\n\n    # Customizing the Box Plot:\n    \n    ax3 = fig.add_subplot(grid[:, 2])\n    # Set title.\n    \n    ax3.set_title('Box Plot')\n    \n    # Plotting the box plot.\n    \n    sns.boxplot(y=feature, data=df, ax=ax3, color=cust_color[0])\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=24))\n\n    plt.suptitle(f'{title}', fontsize=24, fontname = 'monospace', weight='bold')\n    \n    \ndef count_dist(df, colname=None, fixlabel=False, f_axis=None, fixlabel_n=None, fixlabel_txt=None, max_idx=30, fontsize=12, palette=cust_color, rotation=45,\n              title='X distribution', y_label='', shift=-0.005):\n    \"\"\"A function for counting and displaying categorical variables including percentage texts.\"\"\"\n    fig, ax = plt.subplots()\n    sns.barplot(y=df[colname].value_counts().index[:max_idx],\n                x=df[colname].value_counts().values[:max_idx], palette=palette, \n                edgecolor='black', linewidth=1.5, saturation = 1.5)\n    z=df[colname].value_counts().values[:max_idx]\n    for n, i in enumerate(df[colname].value_counts().index[:max_idx]):    \n        ax.text(df[colname].value_counts().values[:max_idx][n]+shift, \n                n, #Y location\n                s=f'{round(z[n]/df.shape[0]*100,1)}%',                 \n                va='center', \n                ha='right', \n                color='white', \n                fontsize=fontsize,\n                bbox=dict(boxstyle='round',facecolor='black', alpha=0.5))\n    if fixlabel:\n        if f_axis == 'x':\n            labels = [item.get_text() for item in ax.get_xticklabels()]\n            labels[fixlabel_n] = fixlabel_txt\n            ax.set_xticklabels(labels)\n        else:\n            labels = [item.get_text() for item in ax.get_yticklabels()]\n            labels[fixlabel_n] = fixlabel_txt\n            ax.set_yticklabels(labels)            \n\n    plt.title(title, fontname = 'monospace', weight='bold')\n    del z\n    \n    plt.yticks(fontsize=12,rotation=rotation)\n    plt.xlabel(\"Count\", fontname = 'monospace', weight='semibold')\n    plt.ylabel(y_label, fontname = 'monospace', weight='semibold')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:22.622123Z","iopub.execute_input":"2022-05-28T21:18:22.623123Z","iopub.status.idle":"2022-05-28T21:18:22.655442Z","shell.execute_reply.started":"2022-05-28T21:18:22.623083Z","shell.execute_reply":"2022-05-28T21:18:22.654640Z"},"trusted":true},"execution_count":56,"outputs":[]},{"cell_type":"code","source":"eli_plot(df, 'Open', 'Open Distribution\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:23.431258Z","iopub.execute_input":"2022-05-28T21:18:23.431989Z","iopub.status.idle":"2022-05-28T21:18:24.439515Z","shell.execute_reply.started":"2022-05-28T21:18:23.431939Z","shell.execute_reply":"2022-05-28T21:18:24.438686Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":"Como se puede observar sigue una distribución no Gaussiana con una media de 8963.54. La densidad es muy pequeña. \n\n### 2.8.2 High:\n\nHigh indica el precio de máximo de ese día.","metadata":{}},{"cell_type":"code","source":"eli_plot(df, 'High', 'High Distribution\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:28.884914Z","iopub.execute_input":"2022-05-28T21:18:28.885283Z","iopub.status.idle":"2022-05-28T21:18:30.195008Z","shell.execute_reply.started":"2022-05-28T21:18:28.885254Z","shell.execute_reply":"2022-05-28T21:18:30.193929Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"Como observamos tampoco sigue una ditribución Gaussiana y la gráfica QQ indica que no los datos varian de la normal.\n\n### 2.8.3 Low:\n\nLow indica el precio mínimo de ese día.","metadata":{}},{"cell_type":"code","source":"eli_plot(df, 'Low', 'Low Distribution\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:30.624891Z","iopub.execute_input":"2022-05-28T21:18:30.625366Z","iopub.status.idle":"2022-05-28T21:18:31.667128Z","shell.execute_reply.started":"2022-05-28T21:18:30.625326Z","shell.execute_reply":"2022-05-28T21:18:31.665889Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"markdown","source":"### 2.8.4 Close:\n\nClose indica el precio de cierre de ese día ajustado por splits","metadata":{}},{"cell_type":"code","source":"eli_plot(df, 'Close', 'Close Distribution\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:36.491193Z","iopub.execute_input":"2022-05-28T21:18:36.491634Z","iopub.status.idle":"2022-05-28T21:18:37.565695Z","shell.execute_reply.started":"2022-05-28T21:18:36.491598Z","shell.execute_reply":"2022-05-28T21:18:37.564621Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"markdown","source":"### 2.8.5 Adj Close:\n\nAdj Close indica el precio de cierre ajustado por splits y distribuciones de dividendos o plusvalías.","metadata":{}},{"cell_type":"code","source":"eli_plot(df, 'Adj Close', 'Adj Close Distribution\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:38.078989Z","iopub.execute_input":"2022-05-28T21:18:38.079832Z","iopub.status.idle":"2022-05-28T21:18:39.126533Z","shell.execute_reply.started":"2022-05-28T21:18:38.079775Z","shell.execute_reply":"2022-05-28T21:18:39.125555Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"markdown","source":"### 2.8.6 Volume:\n\nVolume indica el número físico de acciones negociadas del índice bursátil.","metadata":{}},{"cell_type":"code","source":"eli_plot(df, 'Volume', 'Volume Distribution\\n')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:39.517248Z","iopub.execute_input":"2022-05-28T21:18:39.517676Z","iopub.status.idle":"2022-05-28T21:18:40.557253Z","shell.execute_reply.started":"2022-05-28T21:18:39.517640Z","shell.execute_reply":"2022-05-28T21:18:40.556006Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"### 2.8.7 Target:\n\nTarget: Esta es la variable a predecir. Es una variable binaria","metadata":{}},{"cell_type":"code","source":"# plotting and styling\nfig, ax = plt.subplots(figsize=(12,4))\nsns.barplot(x=df['Target'].value_counts().index,\n            y=df['Target'].value_counts().values,\n            palette=cust_color[::4],\n            edgecolor='black', linewidth=1.5, saturation=1.5)\nplt.xlabel(\"Type of Pollutant\", fontname = 'monospace', weight='semibold')\nplt.ylabel(\"Count\", fontname = 'monospace', weight='semibold')\nplt.title('Target Distribution', fontname = 'monospace', weight='bold');","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:40.945167Z","iopub.execute_input":"2022-05-28T21:18:40.945605Z","iopub.status.idle":"2022-05-28T21:18:41.107773Z","shell.execute_reply.started":"2022-05-28T21:18:40.945560Z","shell.execute_reply":"2022-05-28T21:18:41.107031Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"Vemos que este estudio es un caso de `clasificación binaria balanceada`.","metadata":{}},{"cell_type":"markdown","source":"## 2.9 Estudio del Target \n\nVamos a ver cuáles son las características que tienen mayor correlación con la variable Target:","metadata":{}},{"cell_type":"code","source":"data = df_train.corr().loc[:,['Target']]\nplt.rcParams[\"figure.figsize\"] = (10,3)\n# Fetch Index and Values From Data\nindex = data.index[1:]\nvalues = data.values.flatten()[1:]\n\n# Set figure size, title and labels\nfig,ax = plt.subplots(figsize=(30,8))\nax.set_title(\"Correlación con la variable target\\n\", size=40)\nax.set_xlabel(\"Columns\", size= 30)\nax.set_ylabel(\"\\nCorrelation\", size=30)\n\n# Plot a Barplot\nplot = plt.bar(index,values,color=['red' if x<0 else 'green' for x in values])\n\n# Annotate Plots\nfor p in ax.patches:\n    ax.annotate(\"{:.2f}\".format(p.get_height()),(p.get_x(),p.get_height()))\n\n# Show plot\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:43.591271Z","iopub.execute_input":"2022-05-28T21:18:43.591838Z","iopub.status.idle":"2022-05-28T21:18:43.950080Z","shell.execute_reply.started":"2022-05-28T21:18:43.591797Z","shell.execute_reply":"2022-05-28T21:18:43.948963Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"markdown","source":"Como podemos observar, ninguna característica tiene una correlación significativa con el target.\n\n## 2.10 Estudiar dataset Tweet:\n\nVamos a proceder a eliminar de la parte `text` los emogis, cambiar las mayúsculas por minúsculas entre otras funciones para limpiar el texto y poder clasificarlo.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime, timedelta\nimport requests\nimport pandas as pd\nfrom nltk.stem import WordNetLemmatizer","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:51.416020Z","iopub.execute_input":"2022-05-28T21:18:51.416650Z","iopub.status.idle":"2022-05-28T21:18:51.423156Z","shell.execute_reply.started":"2022-05-28T21:18:51.416604Z","shell.execute_reply":"2022-05-28T21:18:51.421883Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def clean_text(text):\n    text = re.sub(r'@[A-Za-z09]+','', text)\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub(r'#', '', text)\n    text = re.sub(r'https?:\\/\\/?','', text)\n    #text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub(r'\\n','', text)\n    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n    return text\n\ntweet['Tweet_new'] = tweet['text'].apply(clean_text)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:52.443009Z","iopub.execute_input":"2022-05-28T21:18:52.443492Z","iopub.status.idle":"2022-05-28T21:18:52.595028Z","shell.execute_reply.started":"2022-05-28T21:18:52.443458Z","shell.execute_reply":"2022-05-28T21:18:52.593507Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"# Defining dictionary containing all emojis with their meanings.\nemojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', \n          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',\n          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\\\': 'annoyed', \n          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',\n          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',\n          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', \":'-)\": 'sadsmile', ';)': 'wink', \n          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip', '=^.^=': 'cat'}\n","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:53.559275Z","iopub.execute_input":"2022-05-28T21:18:53.559738Z","iopub.status.idle":"2022-05-28T21:18:53.568031Z","shell.execute_reply.started":"2022-05-28T21:18:53.559698Z","shell.execute_reply":"2022-05-28T21:18:53.567242Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def preprocess(textdata):\n    processedText = []\n    \n    # Create Lemmatizer and Stemmer.\n    wordLemm = WordNetLemmatizer()\n    \n    # Defining regex patterns.\n    urlPattern        = r\"((http://)[^ ]*|(https://)[^ ]*|( www\\.)[^ ]*)\"\n    userPattern       = '@[A-Za-z09]+'\n    alphaPattern      = \"[^a-zA-Z0-9]\"\n    sequencePattern   = r\"(.)\\1\\1+\"\n    seqReplacePattern = r\"\\1\\1\"\n    \n    for tweet in textdata:\n        tweet = tweet.lower()\n        \n        # Replace all URls with 'URL'\n        tweet = re.sub(urlPattern,'',tweet)\n        # Replace all emojis.\n        for emoji in emojis.keys():\n            tweet = tweet.replace(emoji, \"EMOJI\" + emojis[emoji])        \n        # Replace @USERNAME to 'USER'.\n        tweet = re.sub(userPattern,'', tweet)        \n        # Replace all non alphabets.\n        tweet = re.sub(alphaPattern, \" \", tweet)\n        # Replace 3 or more consecutive letters by 2 letter.\n        #tweet = re.sub(sequencePattern, seqReplacePattern, tweet)\n\n        tweetwords = ''\n        for word in tweet.split():\n            # Checking if the word is a stopword.\n            #if word not in stopwordlist:\n            if len(word)>1:\n                # Lemmatizing the word.\n                word = wordLemm.lemmatize(word)\n                tweetwords += (word+' ')\n            \n        processedText.append(tweetwords)\n        \n    return processedText","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:54.375397Z","iopub.execute_input":"2022-05-28T21:18:54.376138Z","iopub.status.idle":"2022-05-28T21:18:54.387553Z","shell.execute_reply.started":"2022-05-28T21:18:54.376084Z","shell.execute_reply":"2022-05-28T21:18:54.386419Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import time\nt = time.time()\ntext = tweet['text']\ntweet['text_new'] = preprocess(text)\nprint(f'Text Preprocessing complete.')\nprint(f'Time Taken: {round(time.time()-t)} seconds')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:55.135076Z","iopub.execute_input":"2022-05-28T21:18:55.135607Z","iopub.status.idle":"2022-05-28T21:18:59.961292Z","shell.execute_reply.started":"2022-05-28T21:18:55.135571Z","shell.execute_reply":"2022-05-28T21:18:59.959981Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"tweet.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:18:59.962885Z","iopub.execute_input":"2022-05-28T21:18:59.963278Z","iopub.status.idle":"2022-05-28T21:18:59.977520Z","shell.execute_reply.started":"2022-05-28T21:18:59.963241Z","shell.execute_reply":"2022-05-28T21:18:59.976257Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"markdown","source":"Las columnas que nos interesan son `tweetDate` y `text_new`.","metadata":{}},{"cell_type":"code","source":"tweet = tweet[['tweetDate','text_new']]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:19:02.228669Z","iopub.execute_input":"2022-05-28T21:19:02.229683Z","iopub.status.idle":"2022-05-28T21:19:02.238071Z","shell.execute_reply.started":"2022-05-28T21:19:02.229625Z","shell.execute_reply":"2022-05-28T21:19:02.237373Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"markdown","source":"A continuación, vamos a eliminar unas filas adicionales de la parte de `tweetDate` debido a que no tienen el formato adecuado.","metadata":{}},{"cell_type":"code","source":"tweet = tweet.drop([6930,9664,8498,9632])","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:19:04.379501Z","iopub.execute_input":"2022-05-28T21:19:04.379872Z","iopub.status.idle":"2022-05-28T21:19:04.387835Z","shell.execute_reply.started":"2022-05-28T21:19:04.379842Z","shell.execute_reply":"2022-05-28T21:19:04.386838Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":"A continuación, vamos a cambiar el formato de la caracterítica date para ser más clarificante:","metadata":{}},{"cell_type":"code","source":"weekdayDict = {\"0\": \"M\", \"1\": \"Tu\", \"2\": \"W\", \"3\": \"Th\", \"4\": \"F\", \"5\": \"Sa\", \"6\": \"Su\"}\n\ntweet[\"tweetDate\"] = tweet[\"tweetDate\"].astype('datetime64[ns]')\ntweet[\"hour\"] = tweet[\"tweetDate\"].apply(lambda x: x.hour)\ntweet[\"day\"] = tweet[\"tweetDate\"].apply(lambda x: x.weekday())\ntweet[\"dayofmonth\"] = tweet[\"tweetDate\"].apply(lambda x: x.day)\ntweet[\"month\"] = tweet[\"tweetDate\"].apply(lambda x: x.month)\ntweet[\"date\"] = tweet['tweetDate'].dt.date","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:19:06.442582Z","iopub.execute_input":"2022-05-28T21:19:06.443645Z","iopub.status.idle":"2022-05-28T21:19:09.026465Z","shell.execute_reply.started":"2022-05-28T21:19:06.443587Z","shell.execute_reply":"2022-05-28T21:19:09.024907Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"tweet.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:19:20.621313Z","iopub.execute_input":"2022-05-28T21:19:20.621734Z","iopub.status.idle":"2022-05-28T21:19:20.636475Z","shell.execute_reply.started":"2022-05-28T21:19:20.621698Z","shell.execute_reply":"2022-05-28T21:19:20.635777Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"code","source":"def get_polarity(text):\n    analysis = TextBlob(text)\n    if text!= '':\n        result = analysis.translate(from_lang = 'es', to = 'en').sentiment.polarity","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:22:44.146225Z","iopub.execute_input":"2022-05-28T21:22:44.146592Z","iopub.status.idle":"2022-05-28T21:22:44.151755Z","shell.execute_reply.started":"2022-05-28T21:22:44.146562Z","shell.execute_reply":"2022-05-28T21:22:44.150490Z"},"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"tweet['polarity'] = tweet['text_new'].apply(get_polarity)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# 3. Algoritmo:\n\nEn esta parte del proyecto, vamos a diseñar un algoritmo tipo clasificatorio binario para datos balanceados. Para ello vamos a usar `Bottleneck-Autoencoder`. La idea general será comprimir el espacio de características, permitiendo que el modelo aprenda características más significativas y robustas. La incrustación se añadirá al conjunto de datos original, alimentando la antigua red neuronal totalmente conectada para crear la predicción final. \n\nPara mayor claridad vamos a esquematizar el algoritmo por pasos:\n\n- `Paso 1`: Seleccionar las caracteríticas a usar:","metadata":{}},{"cell_type":"code","source":"train_df = df[['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume',\n       'EMA5', 'EMA20','Target']]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:23:34.430180Z","iopub.execute_input":"2022-05-28T21:23:34.430550Z","iopub.status.idle":"2022-05-28T21:23:34.437380Z","shell.execute_reply.started":"2022-05-28T21:23:34.430519Z","shell.execute_reply":"2022-05-28T21:23:34.436412Z"},"trusted":true},"execution_count":76,"outputs":[]},{"cell_type":"code","source":"df_test.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:26:31.579101Z","iopub.execute_input":"2022-05-28T21:26:31.579992Z","iopub.status.idle":"2022-05-28T21:26:31.586554Z","shell.execute_reply.started":"2022-05-28T21:26:31.579950Z","shell.execute_reply":"2022-05-28T21:26:31.585452Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"df_test_sin_index = df_test[['Open', 'High', 'Low', 'Close', 'Adj Close',\n       'Volume', 'EMA5', 'EMA20']]","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:26:53.551148Z","iopub.execute_input":"2022-05-28T21:26:53.551573Z","iopub.status.idle":"2022-05-28T21:26:53.558001Z","shell.execute_reply.started":"2022-05-28T21:26:53.551537Z","shell.execute_reply":"2022-05-28T21:26:53.557231Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"markdown","source":"- `Paso 2`: Dividir nuestro dataset:","metadata":{}},{"cell_type":"code","source":"# split dataframes for later modeling\nX = train_df.drop(['Target'], axis=1).copy()\ny = train_df['Target'].copy()\n\nX_test = df_test_sin_index.copy()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:26:56.442293Z","iopub.execute_input":"2022-05-28T21:26:56.443468Z","iopub.status.idle":"2022-05-28T21:26:56.449822Z","shell.execute_reply.started":"2022-05-28T21:26:56.443415Z","shell.execute_reply":"2022-05-28T21:26:56.449191Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"code","source":"print(X.shape, y.shape,X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:26:57.554405Z","iopub.execute_input":"2022-05-28T21:26:57.555259Z","iopub.status.idle":"2022-05-28T21:26:57.561264Z","shell.execute_reply.started":"2022-05-28T21:26:57.555187Z","shell.execute_reply":"2022-05-28T21:26:57.559734Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"- `Paso 3`: ","metadata":{}},{"cell_type":"code","source":"# define helper functions\ndef set_seed(seed):\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    print(f\"Seed set to: {seed}\")\n\ndef plot_eval_results(scores, n_splits):\n    cols = 5\n    rows = int(np.ceil(n_splits/cols))\n    \n    fig, ax = plt.subplots(rows, cols, tight_layout=True, figsize=(20,2.5))\n    ax = ax.flatten()\n\n    for fold in range(len(scores)):\n        df_eval = pd.DataFrame({'train_loss': scores[fold]['loss'], 'valid_loss': scores[fold]['val_loss']})\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['train_loss'],\n            label='train_loss',\n            ax=ax[fold]\n        )\n\n        sns.lineplot(\n            x=df_eval.index,\n            y=df_eval['valid_loss'],\n            label='valid_loss',\n            ax=ax[fold]\n        )\n\n        ax[fold].set_ylabel('')\n\n    sns.despine()\n\ndef plot_cm(cm):\n    metrics = {\n        'accuracy': cm / cm.sum(),\n        'recall' : cm / cm.sum(axis=1),\n        'precision': cm / cm.sum(axis=0)\n    }\n    \n    fig, ax = plt.subplots(1,3, tight_layout=True, figsize=(15,5))\n    ax = ax.flatten()\n\n    mask = (np.eye(cm.shape[0]) == 0) * 1\n\n    for idx, (name, matrix) in enumerate(metrics.items()):\n\n        ax[idx].set_title(name)\n\n        sns.heatmap(\n            data=matrix,\n            cmap=sns.dark_palette(\"#69d\", reverse=True, as_cmap=True),\n            cbar=False,\n            mask=mask,\n            lw=0.25,\n            annot=True,\n            fmt='.2f',\n            ax=ax[idx]\n        )\n    sns.despine()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:27:02.222019Z","iopub.execute_input":"2022-05-28T21:27:02.223254Z","iopub.status.idle":"2022-05-28T21:27:02.235070Z","shell.execute_reply.started":"2022-05-28T21:27:02.223202Z","shell.execute_reply":"2022-05-28T21:27:02.233711Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"markdown","source":"- `Paso 4`: difinir los callbacks: ReduceLROnPlateau y EarlyStopping","metadata":{}},{"cell_type":"code","source":"# define callbacks\nlr = keras.callbacks.ReduceLROnPlateau(\n    monitor=\"val_loss\", \n    factor=0.5, \n    patience=5, \n    verbose=True\n)\n\nes = keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\", \n    patience=10, \n    verbose=True, \n    mode=\"min\", \n    restore_best_weights=True\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:27:04.457663Z","iopub.execute_input":"2022-05-28T21:27:04.458177Z","iopub.status.idle":"2022-05-28T21:27:04.464715Z","shell.execute_reply.started":"2022-05-28T21:27:04.458133Z","shell.execute_reply":"2022-05-28T21:27:04.463804Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"- `Paso 5`: Crear los autoencoders:","metadata":{}},{"cell_type":"code","source":"# create autoencoder\nclass EncodingLayer(layers.Layer):\n    def __init__(self, encoding_dim, activation='relu'):\n        super().__init__()\n        self.enc1 = layers.Dense(encoding_dim*8, activation)\n        self.enc2 = layers.Dense(encoding_dim*4, activation)\n        self.enc3 = layers.Dense(encoding_dim, activation)\n    \n    def call(self, inputs):\n        x = self.enc1(inputs)\n        x = self.enc2(x)\n        x = self.enc3(x)\n        return x\n\nclass DecodingLayer(layers.Layer):\n    def __init__(self, encoding_dim, num_outputs, activation='relu'):\n        super().__init__()\n        self.dec1 = layers.Dense(encoding_dim*4, activation)\n        self.dec2 = layers.Dense(encoding_dim*8, activation)\n        self.dec3 = layers.Dense(num_outputs, activation='linear')\n    \n    def call(self, inputs):\n        x = self.dec1(inputs)\n        x = self.dec2(x)\n        x = self.dec3(x)\n        return x\n    \nclass AutoEncoder(keras.Model):\n    def __init__(self, encoding_dim, num_outputs, activation='relu'):\n        super().__init__()\n        self.encoder = EncodingLayer(encoding_dim, activation,)\n        self.decoder = DecodingLayer(encoding_dim, num_outputs)\n    \n    def call(self, inputs):\n        encoder = self.encoder(inputs)\n        decoder = self.decoder(encoder)\n        return decoder\n    \n    def get_encoder(self):\n        return self.encoder","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:27:06.422165Z","iopub.execute_input":"2022-05-28T21:27:06.423445Z","iopub.status.idle":"2022-05-28T21:27:06.440086Z","shell.execute_reply.started":"2022-05-28T21:27:06.423390Z","shell.execute_reply":"2022-05-28T21:27:06.438526Z"},"trusted":true},"execution_count":96,"outputs":[]},{"cell_type":"markdown","source":"- `Paso 6`: Creac capas personalizadas con la clase `DenseBlock` y después conectaremos la red neuronal con la clase `MLP`.","metadata":{}},{"cell_type":"code","source":"# create custom layer\nclass DenseBlock(layers.Layer):\n    def __init__(self, units, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.dense = layers.Dense(\n            units, activation, \n            kernel_initializer=\"lecun_normal\", \n            kernel_regularizer=keras.regularizers.l2(l2)\n        )\n        self.batchn = layers.BatchNormalization()\n        self.dropout = layers.Dropout(dropout_rate)\n    \n    def call(self, inputs):\n        x = self.dense(inputs)\n        x = self.batchn(x)\n        x = self.dropout(x)\n        return x\n\n# create fully-connected NN\nclass MLP(keras.Model):\n    def __init__(self, hidden_layers, autoencoder, activation='relu', dropout_rate=0, l2=0):\n        super().__init__()\n        self.encoder = autoencoder.get_encoder()\n        self.hidden_layers = [DenseBlock(units, activation, l2) for units in hidden_layers]\n        self.softmax = layers.Dense(units=target.shape[-1], activation='softmax')\n        self.concat = layers.Concatenate()\n        \n    def call(self, inputs):\n        encoding = self.encoder(inputs)\n        x = self.concat([inputs, encoding])\n        for layer in self.hidden_layers:\n            x = layer(x)\n        x = self.softmax(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:27:08.408741Z","iopub.execute_input":"2022-05-28T21:27:08.409189Z","iopub.status.idle":"2022-05-28T21:27:08.422317Z","shell.execute_reply.started":"2022-05-28T21:27:08.409151Z","shell.execute_reply":"2022-05-28T21:27:08.421589Z"},"trusted":true},"execution_count":97,"outputs":[]},{"cell_type":"markdown","source":"- `Paso 7`: Indicaremos las excepciones del algoritmo.","metadata":{}},{"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n    tf_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    print(\"Running on TPU:\", tpu.master())\nexcept:\n    tf_strategy = tf.distribute.get_strategy()\n    print(f\"Running on {tf_strategy.num_replicas_in_sync} replicas\")\n    print(\"Number of GPUs Available: \", len(tf.config.list_physical_devices('GPU')))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:27:10.390373Z","iopub.execute_input":"2022-05-28T21:27:10.391239Z","iopub.status.idle":"2022-05-28T21:27:10.397523Z","shell.execute_reply.started":"2022-05-28T21:27:10.391187Z","shell.execute_reply":"2022-05-28T21:27:10.396646Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"markdown","source":"- `Paso 8`: aplicaremos el cuerpo del algoritmo para predicir el target.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import RobustScaler, LabelEncoder","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:33:54.483851Z","iopub.execute_input":"2022-05-28T21:33:54.484800Z","iopub.status.idle":"2022-05-28T21:33:54.489590Z","shell.execute_reply.started":"2022-05-28T21:33:54.484761Z","shell.execute_reply":"2022-05-28T21:33:54.488209Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"le = LabelEncoder()\ntarget = keras.utils.to_categorical(le.fit_transform(y))\nseed = 2022\nset_seed(seed)\n\ncv = StratifiedKFold(n_splits=20, shuffle=True, random_state=1)\n\npredictions = []\noof_preds = {'y_valid': list(), 'y_hat': list()}\n\nscores_ae = {fold:None for fold in range(cv.n_splits)}\nscores_nn = {fold:None for fold in range(cv.n_splits)}\n\nfor fold, (idx_train, idx_valid) in enumerate(cv.split(X,y)):\n    X_train, y_train = X.iloc[idx_train], target[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], target[idx_valid]\n\n    # scale data\n    scl = RobustScaler()\n    X_train = scl.fit_transform(X_train)\n    X_valid = scl.transform(X_valid)\n\n    # train autoencoder\n    with tf_strategy.scope():\n        ae = AutoEncoder(\n            encoding_dim=4,\n            num_outputs=X.shape[-1],\n            activation='relu'\n        )\n\n        ae.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.MeanSquaredError()\n        )\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || Autoencoder Training\")\n\n    history_ae = ae.fit(\n        X_train, X_train,\n        validation_data=(X_valid, X_valid),\n        epochs=100,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n\n    scores_ae[fold] = history_ae.history\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || AE Min Val Loss: {np.min(scores_ae[fold]['val_loss'])}\")\n\n    # train fully-connected nn\n    with tf_strategy.scope():\n        model = MLP(\n            hidden_layers=[384, 256, 128, 64],\n            autoencoder=ae,\n            activation='selu',\n        )\n\n        model.compile(\n            optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n            loss=keras.losses.BinaryCrossentropy(),\n            metrics=['acc']\n        )\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || NN Training\")\n\n    history_nn = model.fit(\n        X_train, y_train,\n        validation_data=(X_valid, y_valid),\n        epochs=500,\n        batch_size=4096,\n        shuffle=True,\n        verbose=False,\n        callbacks=[lr,es]\n    )\n\n    scores_nn[fold] = history_nn.history\n\n    oof_preds['y_valid'].extend(y.iloc[idx_valid])\n    oof_preds['y_hat'].extend(model.predict(X_valid, batch_size=4096))\n\n    prediction = model.predict(scl.transform(X_test), batch_size=4096)\n    predictions.append(prediction)\n\n    del ae, model\n    gc.collect()\n    K.clear_session()\n\n    print('_'*65)\n    print(f\"Fold {fold+1} || NN Min Val Loss: {np.min(scores_nn[fold]['val_loss'])}\")\n\noverall_score_ae = [np.min(scores_ae[fold]['val_loss']) for fold in range(cv.n_splits)]\noverall_score_nn = [np.min(scores_nn[fold]['val_loss']) for fold in range(cv.n_splits)]\n\nprint('_'*65)\nprint(f\"Overall AE Mean Validation Loss: {np.mean(overall_score_ae)} || Overall NN Mean Validation Loss: {np.mean(overall_score_nn)}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:34:16.614489Z","iopub.execute_input":"2022-05-28T21:34:16.614929Z","iopub.status.idle":"2022-05-28T21:42:24.191056Z","shell.execute_reply.started":"2022-05-28T21:34:16.614893Z","shell.execute_reply":"2022-05-28T21:42:24.190105Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"# 4. Evaluación del modelo\n\nVamos a visualizar los resultados de las funciones de pérdida","metadata":{}},{"cell_type":"code","source":"plot_eval_results(scores_nn, cv.n_splits)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:42:49.824941Z","iopub.execute_input":"2022-05-28T21:42:49.825341Z","iopub.status.idle":"2022-05-28T21:42:52.473940Z","shell.execute_reply.started":"2022-05-28T21:42:49.825312Z","shell.execute_reply":"2022-05-28T21:42:52.472867Z"},"trusted":true},"execution_count":105,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a ver, las métricas del modelo para averiguar si el algoritmo a predicho correctamente.","metadata":{}},{"cell_type":"code","source":"# prepare oof_predictions\noof_y_true = np.array(oof_preds['y_valid'])\noof_y_hat = le.inverse_transform(np.argmax(oof_preds['y_hat'], axis=1))\n\n# create confusion matrix, calculate accuracy, recall & precision\ncm = pd.DataFrame(data=confusion_matrix(oof_y_true, oof_y_hat, labels=le.classes_), index=le.classes_, columns=le.classes_)\nplot_cm(cm)","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:42:55.240208Z","iopub.execute_input":"2022-05-28T21:42:55.240838Z","iopub.status.idle":"2022-05-28T21:42:55.624482Z","shell.execute_reply.started":"2022-05-28T21:42:55.240801Z","shell.execute_reply":"2022-05-28T21:42:55.623636Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"markdown","source":"Como podemos ver, el `recall`para cuando Target = 1 es del 61% pero si miramos el caso Target = 0 es del 43%. En el Accuracy y la Precisión tienen valores parecidos. La conclusión es que no ha hecho una buena predicción del modelo. Vamos a ver el informe de la matriz de confusión:","metadata":{}},{"cell_type":"code","source":"cm = confusion_matrix(oof_y_true, oof_y_hat)\nix = np.arange(cm.shape[0])\ncol_names = [f'Target={cls}' for cls in le.classes_]\ncm = pd.DataFrame(cm, columns=col_names, index=col_names)\nsns.heatmap(cm, cmap='Blues', annot=True, fmt='d').set(title=f'Matriz de confusión\\n');","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:43:17.864982Z","iopub.execute_input":"2022-05-28T21:43:17.865396Z","iopub.status.idle":"2022-05-28T21:43:18.036848Z","shell.execute_reply.started":"2022-05-28T21:43:17.865364Z","shell.execute_reply":"2022-05-28T21:43:18.036239Z"},"trusted":true},"execution_count":107,"outputs":[]},{"cell_type":"markdown","source":"Como se puede observar, no se ha predicho de forma adecuada. Deberíamos intentar, para próximos estudios, aplicar diferentes ingeniería de características.","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import classification_report","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:43:23.014515Z","iopub.execute_input":"2022-05-28T21:43:23.015329Z","iopub.status.idle":"2022-05-28T21:43:23.019562Z","shell.execute_reply.started":"2022-05-28T21:43:23.015294Z","shell.execute_reply":"2022-05-28T21:43:23.018233Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"code","source":"print(classification_report(oof_y_true, oof_y_hat))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:43:23.964105Z","iopub.execute_input":"2022-05-28T21:43:23.964480Z","iopub.status.idle":"2022-05-28T21:43:23.981892Z","shell.execute_reply.started":"2022-05-28T21:43:23.964450Z","shell.execute_reply":"2022-05-28T21:43:23.980700Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"markdown","source":"Ahora vamos a ver los resultados:","metadata":{}},{"cell_type":"code","source":"#create final prediction, inverse labels to original classes\nfinal_predictions = le.inverse_transform(np.argmax(sum(predictions), axis=1))","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:43:28.478906Z","iopub.execute_input":"2022-05-28T21:43:28.480161Z","iopub.status.idle":"2022-05-28T21:43:28.486058Z","shell.execute_reply.started":"2022-05-28T21:43:28.480114Z","shell.execute_reply":"2022-05-28T21:43:28.485254Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"df_test.columns","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:45:04.606326Z","iopub.execute_input":"2022-05-28T21:45:04.606773Z","iopub.status.idle":"2022-05-28T21:45:04.614765Z","shell.execute_reply.started":"2022-05-28T21:45:04.606736Z","shell.execute_reply":"2022-05-28T21:45:04.613789Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['test_index'] = df_test['test_index']\nsubmission['Prediction'] = final_predictions\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:45:20.506968Z","iopub.execute_input":"2022-05-28T21:45:20.507793Z","iopub.status.idle":"2022-05-28T21:45:20.518590Z","shell.execute_reply.started":"2022-05-28T21:45:20.507752Z","shell.execute_reply":"2022-05-28T21:45:20.518008Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('predictions.csv', index=False)\nsubmission.to_json('predictions.json')","metadata":{"execution":{"iopub.status.busy":"2022-05-28T21:45:27.705700Z","iopub.execute_input":"2022-05-28T21:45:27.706654Z","iopub.status.idle":"2022-05-28T21:45:27.717727Z","shell.execute_reply.started":"2022-05-28T21:45:27.706581Z","shell.execute_reply":"2022-05-28T21:45:27.716624Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"markdown","source":"# 5. Conclusiones\n\nHemos realizado un análisis exaustivo de variables y hemos incorporado indicadores de análisis técnico de bolsa para poder averiguar tendencias, cambios de tendencia y hasta señales de compra y venta. \nDespués de desarrollar un algoritmo basado en Autoencoders con keras, hemos podido ver que la clasificación es muy dificultosa. Esto es una muestra más de que predecir los datos a teniendo información de 3 días anteriores es muy dificil, por lo que la mayoría de traders profesionales usan indicadores del cambio de tendencia en periodos de 1 mes, trimestre, semestre... para tener mejor perspetiva y fiabilidad al momento de comprar y vendes acciones en el IBEX35.","metadata":{}},{"cell_type":"markdown","source":"# 6. Referencias: ","metadata":{}},{"cell_type":"markdown","source":"[1] https://www.expansion.com/mercados/2015/07/13/55a2ab7546163f7c088b4579.html\n\n","metadata":{}}]}